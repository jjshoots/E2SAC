flatpak: /share/apps/Anaconda3_2020.11/lib/libuuid.so.1: no version information available (required by /usr/lib64/libappstream-glib.so.8)
flatpak: /share/apps/Anaconda3_2020.11/lib/libuuid.so.1: no version information available (required by /usr/lib64/libblkid.so.1)

CommandNotFoundError: Your shell has not been properly configured to use 'conda activate'.
To initialize your shell, run

    $ conda init <SHELL_NAME>

Currently supported shells are:
  - bash
  - fish
  - tcsh
  - xonsh
  - zsh
  - powershell

See 'conda init --help' for more information and options.

IMPORTANT: You may need to close and restart your shell after running 'conda init'.


wandb: Starting wandb agent üïµÔ∏è
2021-10-23 13:15:28,550 - wandb.wandb_agent - INFO - Running runs: []
2021-10-23 13:15:28,805 - wandb.wandb_agent - INFO - Agent received command: run
2021-10-23 13:15:28,805 - wandb.wandb_agent - INFO - Agent starting run with config:
	ac_update_ratio: 5
	confidence_scale: 8.766202816886258
	target_entropy: 1.7137670698316954
	use_entropy: True
2021-10-23 13:15:28,877 - wandb.wandb_agent - INFO - About to run command: python3 src/main.py --train --wandb
2021-10-23 13:15:33,888 - wandb.wandb_agent - INFO - Running runs: ['pjbw69na']
wandb: Currently logged in as: jjshoots (use `wandb login --relogin` to force relogin)
wandb: WARNING Ignored wandb.init() arg project when running a sweep
wandb: WARNING Ignored wandb.init() arg entity when running a sweep
-----------------------
Using Device cuda:0
-----------------------
wandb: wandb version 0.12.5 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.0
wandb: Syncing run 40351
wandb:  View project at https://wandb.ai/jjshoots/UA3SAC_gym
wandb:  View sweep at https://wandb.ai/jjshoots/UA3SAC_gym/sweeps/nktx8eqi
wandb:  View run at https://wandb.ai/jjshoots/UA3SAC_gym/runs/pjbw69na
wandb: Run data is saved locally in /home/taij/e2sac/wandb/run-20211023_131543-pjbw69na
wandb: Run `wandb offline` to turn off syncing.
wandb: Network error (ConnectionError), entering retry loop.
/home/taij/e2sac/src/ai_lib/UASAC.py:114: UserWarning: Target entropy is recommended to be negative,                                  currently it is 1.7137670698316954,                                  I hope you know what you're doing...
  warnings.warn(f"Target entropy is recommended to be negative,\
/home/taij/.conda/envs/robocar/lib/python3.8/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)
  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)
[34m[1mwandb[0m: Network error resolved after 0:00:38.439841, resuming normal operation.
502 response executing GraphQL.

<html><head>
<meta http-equiv="content-type" content="text/html;charset=utf-8">
<title>502 Server Error</title>
</head>
<body text=#000000 bgcolor=#ffffff>
<h1>Error: Server Error</h1>
<h2>The server encountered a temporary error and could not complete your request.<p>Please try again in 30 seconds.</h2>
<h2></h2>
</body></html>


No weights directory for weights/Version40351, generating new one in 3 seconds.
No weights directory for optim_weights/Version40351, generating new one in 3 seconds.
No weights file found, generating new one during training.
Epoch 1; Batch Number 0; Running Loss -0.89953; Lowest Running Loss -0.89953
Epoch 2; Batch Number 0; Running Loss -6.39917; Lowest Running Loss -0.89953
New lowest point, saving weights to: weights/weights0.pth
Epoch 3; Batch Number 0; Running Loss -47.50410; Lowest Running Loss -6.39917
New lowest point, saving weights to: weights/weights1.pth
Epoch 4; Batch Number 0; Running Loss -52.00777; Lowest Running Loss -47.50410
New lowest point, saving weights to: weights/weights2.pth
Epoch 5; Batch Number 0; Running Loss -34.67019; Lowest Running Loss -52.00777
Epoch 6; Batch Number 0; Running Loss -80.30915; Lowest Running Loss -52.00777
New lowest point, saving weights to: weights/weights3.pth
Epoch 7; Batch Number 0; Running Loss -40.00671; Lowest Running Loss -80.30915
Epoch 8; Batch Number 0; Running Loss -7.72184; Lowest Running Loss -80.30915
Epoch 9; Batch Number 0; Running Loss -27.94530; Lowest Running Loss -80.30915
Epoch 10; Batch Number 0; Running Loss -23.29815; Lowest Running Loss -80.30915
Epoch 11; Batch Number 0; Running Loss -35.25464; Lowest Running Loss -80.30915
Passed 5 intervals without saving so far, saving weights to: /weights_intermediary.pth
Epoch 12; Batch Number 0; Running Loss -17.46432; Lowest Running Loss -80.30915
Epoch 13; Batch Number 0; Running Loss -28.41832; Lowest Running Loss -80.30915
Epoch 14; Batch Number 0; Running Loss -38.10934; Lowest Running Loss -80.30915
Epoch 15; Batch Number 0; Running Loss -46.89024; Lowest Running Loss -80.30915
Epoch 16; Batch Number 0; Running Loss -48.90724; Lowest Running Loss -80.30915
Passed 5 intervals without saving so far, saving weights to: /weights_intermediary.pth
Epoch 17; Batch Number 0; Running Loss -109.26566; Lowest Running Loss -80.30915
New lowest point, saving weights to: weights/weights4.pth
Epoch 18; Batch Number 0; Running Loss -151.62977; Lowest Running Loss -109.26566
New lowest point, saving weights to: weights/weights5.pth
Epoch 19; Batch Number 0; Running Loss -215.82718; Lowest Running Loss -151.62977
New lowest point, saving weights to: weights/weights6.pth
Epoch 20; Batch Number 0; Running Loss -419.67964; Lowest Running Loss -215.82718
New lowest point, saving weights to: weights/weights7.pth
Epoch 21; Batch Number 0; Running Loss -198.71329; Lowest Running Loss -419.67964
Epoch 22; Batch Number 0; Running Loss -392.78521; Lowest Running Loss -419.67964
Epoch 23; Batch Number 0; Running Loss -433.44306; Lowest Running Loss -419.67964
New lowest point, saving weights to: weights/weights8.pth
Epoch 24; Batch Number 0; Running Loss -412.80032; Lowest Running Loss -433.44306
Epoch 25; Batch Number 0; Running Loss -530.96464; Lowest Running Loss -433.44306
New lowest point, saving weights to: weights/weights9.pth
Epoch 26; Batch Number 0; Running Loss -523.02729; Lowest Running Loss -530.96464
Epoch 27; Batch Number 0; Running Loss -523.35359; Lowest Running Loss -530.96464
Epoch 28; Batch Number 0; Running Loss -559.97878; Lowest Running Loss -530.96464
New lowest point, saving weights to: weights/weights10.pth
Epoch 29; Batch Number 0; Running Loss -689.66011; Lowest Running Loss -559.97878
New lowest point, saving weights to: weights/weights11.pth
Epoch 30; Batch Number 0; Running Loss -598.80649; Lowest Running Loss -689.66011
Epoch 31; Batch Number 0; Running Loss -559.64470; Lowest Running Loss -689.66011
Epoch 32; Batch Number 0; Running Loss -745.35497; Lowest Running Loss -689.66011
New lowest point, saving weights to: weights/weights12.pth
Epoch 33; Batch Number 0; Running Loss -693.91156; Lowest Running Loss -745.35497
Epoch 34; Batch Number 0; Running Loss -676.42791; Lowest Running Loss -745.35497
Epoch 35; Batch Number 0; Running Loss -626.73613; Lowest Running Loss -745.35497
Epoch 36; Batch Number 0; Running Loss -602.71057; Lowest Running Loss -745.35497
Epoch 37; Batch Number 0; Running Loss -551.16198; Lowest Running Loss -745.35497
Passed 5 intervals without saving so far, saving weights to: /weights_intermediary.pth
Epoch 38; Batch Number 0; Running Loss -439.95537; Lowest Running Loss -745.35497
Epoch 39; Batch Number 0; Running Loss -597.69422; Lowest Running Loss -745.35497
Epoch 40; Batch Number 0; Running Loss -511.53995; Lowest Running Loss -745.35497
Epoch 41; Batch Number 0; Running Loss -607.31634; Lowest Running Loss -745.35497
Epoch 42; Batch Number 0; Running Loss -506.75376; Lowest Running Loss -745.35497
Passed 5 intervals without saving so far, saving weights to: /weights_intermediary.pth
Epoch 43; Batch Number 0; Running Loss -732.78405; Lowest Running Loss -745.35497
Epoch 44; Batch Number 0; Running Loss -609.13431; Lowest Running Loss -745.35497
Epoch 45; Batch Number 0; Running Loss -677.70025; Lowest Running Loss -745.35497
Epoch 46; Batch Number 0; Running Loss -597.08208; Lowest Running Loss -745.35497
Epoch 47; Batch Number 0; Running Loss -602.27839; Lowest Running Loss -745.35497
Passed 5 intervals without saving so far, saving weights to: /weights_intermediary.pth
Epoch 48; Batch Number 0; Running Loss -458.70408; Lowest Running Loss -745.35497
Epoch 49; Batch Number 0; Running Loss -271.04841; Lowest Running Loss -745.35497
wandb: Waiting for W&B process to finish, PID 244244
wandb: Program ended successfully.
wandb: - 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb: \ 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb: | 0.00MB of 0.01MB uploaded (0.00MB deduped)wandb: / 0.00MB of 0.01MB uploaded (0.00MB deduped)wandb: - 0.00MB of 0.01MB uploaded (0.00MB deduped)wandb: \ 0.01MB of 0.01MB uploaded (0.00MB deduped)wandb: | 0.01MB of 0.01MB uploaded (0.00MB deduped)wandb: / 0.01MB of 0.01MB uploaded (0.00MB deduped)wandb: - 0.01MB of 0.01MB uploaded (0.00MB deduped)wandb: \ 0.01MB of 0.01MB uploaded (0.00MB deduped)wandb: | 0.01MB of 0.01MB uploaded (0.00MB deduped)wandb: / 0.01MB of 0.01MB uploaded (0.00MB deduped)wandb:                                                                                
wandb: Find user logs for this run at: /home/taij/e2sac/wandb/run-20211023_131543-pjbw69na/logs/debug.log
wandb: Find internal logs for this run at: /home/taij/e2sac/wandb/run-20211023_131543-pjbw69na/logs/debug-internal.log
wandb: Run summary:
wandb:          epoch 49
wandb:    mean_reward 0.58643
wandb:   total_reward 271.04841
wandb:   mean_entropy 1.812
wandb:      sup_scale 0.25188
wandb:      log_alpha -3.90172
wandb:   num_episodes 1345
wandb:       _runtime 410603
wandb:     _timestamp 1635401946
wandb:          _step 152359
wandb: Run history:
wandb:          epoch ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà
wandb:    mean_reward ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñÖ
wandb:   total_reward ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÖ‚ñÉ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñÜ‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÖ‚ñá‚ñÜ‚ñá‚ñÜ‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÖ‚ñÉ
wandb:   mean_entropy ‚ñÉ‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñÖ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà
wandb:      sup_scale ‚ñà‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÜ‚ñÉ
wandb:      log_alpha ‚ñÖ‚ñà‚ñà‚ñá‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ
wandb:   num_episodes ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà
wandb:       _runtime ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà
wandb:     _timestamp ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà
wandb:          _step ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà
wandb: 
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: 
wandb: Synced 40351: https://wandb.ai/jjshoots/UA3SAC_gym/runs/pjbw69na

2021-10-28 07:19:16,400 - wandb.wandb_agent - INFO - Cleaning up finished run: pjbw69na
wandb: Terminating and syncing runs. Press ctrl-c to kill.
